# -*- coding: utf-8 -*-
"""aula15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LqRPZNGiGFu3Z-HXDPu7xmsmdsz_sKzN
"""

!pip install PySpark

import pyspark
from pyspark.sql import SparkSession
from pyspark import SparkFiles

spark = SparkSession.builder.appName("Wordcount").master("local[*]").getOrCreate()
sc = spark.sparkContext

rdd = sc.textFile("Seinfeld.txt")

rdd_fiatten = rdd.flatMap(lambda x: x.lower().split(" "))
rdd_fiatten.take(5)

#criando no formato (palvara, 1)
pairRDD = rdd_fiatten.map(lambda x: (x,1))
pairRDD.take(5)

#somar as ocorrencias por chave
wordcount = pairRDD.reduceByKey(lambda x,y: x+y)
wordcount.take(5)

#ordenanar pela mais frequente
wordcount_sorted = wordcount.sortBy(lambda x: x[1], ascending=False)
wordcount_sorted.take(5)